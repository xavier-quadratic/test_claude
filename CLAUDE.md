# CLAUDE.md - AI Assistant Guide

This document provides comprehensive guidance for AI assistants working with this codebase.

## Project Overview

**Project Name:** Quadratic Labs Web Scraper
**Repository:** xavier-quadratic/test_claude
**Language:** Python 3.7+
**Purpose:** Web scraping tool for quadratic-labs.com that extracts site structure, builds hierarchical trees, and exports results in multiple formats.

### Key Features
- Automated web scraping with BFS (Breadth-First Search) traversal
- Site structure visualization with colored tree output
- Multiple export formats (JSON, TXT)
- Hierarchical relationship tracking (parent-child)
- Depth analysis and statistics
- Respectful scraping with rate limiting

## Codebase Structure

```
test_claude/
├── scraper.py              # Main scraper implementation (336 lines)
├── requirements.txt        # Python dependencies
├── readme.md              # User-facing documentation (French)
├── .gitignore             # Git ignore patterns
└── [Output files]         # Generated by scraper
    ├── quadratic_labs_pages.json
    ├── quadratic_labs_pages.txt
    └── quadratic_labs_tree.json
```

### Architecture Overview

**Single-file architecture:** All scraping logic is contained in `scraper.py`.

**Main Components:**
1. `QuadraticLabsScraper` class (lines 26-287) - Core scraper with all functionality
2. `main()` function (lines 289-332) - CLI entry point
3. Module can be used as standalone script or imported as library

## Key Files Reference

### scraper.py (lines 1-336)

**Core Class: `QuadraticLabsScraper`**

Constructor parameters:
- `base_url` (str): Target website URL (default: "https://quadratic-labs.com")
- `use_colors` (bool): Enable/disable ANSI color output (default: True)

**Key Methods:**

| Method | Lines | Purpose |
|--------|-------|---------|
| `scrape()` | 84-159 | Main scraping logic using BFS traversal |
| `get_links_from_page()` | 56-82 | Extract links from HTML page |
| `save_to_json()` | 161-172 | Export flat URL list as JSON |
| `save_to_txt()` | 174-180 | Export flat URL list as text |
| `save_tree_to_json()` | 198-205 | Export hierarchical tree structure |
| `print_tree()` | 260-271 | Display colored ASCII tree in terminal |
| `get_tree_stats()` | 273-286 | Calculate depth statistics |
| `is_valid_url()` | 42-45 | Validate URLs belong to same domain |
| `normalize_url()` | 47-54 | Standardize URLs (remove fragments, trailing slashes) |

**Data Structures:**

```python
self.visited_urls: Set[str]           # Already scraped URLs
self.found_urls: Set[str]             # All discovered URLs
self.url_hierarchy: Dict[str, Dict]   # Parent-child relationships
    # Structure: {url: {'parent': str, 'children': List[str], 'depth': int}}
self.url_depth: Dict[str, int]        # URL -> depth mapping
```

**Color Scheme (lines 207-221):**
- Depth 0 (root): Cyan + Bright
- Depth 1: Green
- Depth 2: Yellow
- Depth 3: Magenta
- Depth 4+: Red
- Connectors/info: Dim gray

### requirements.txt

Dependencies with locked versions:
- `requests==2.31.0` - HTTP requests
- `beautifulsoup4==4.12.3` - HTML parsing
- `lxml==5.1.0` - Fast XML/HTML parser
- `colorama==0.4.6` - Cross-platform ANSI colors (Windows/Linux/macOS support)

### .gitignore

Excludes:
- Scraper output files (`quadratic_labs_*.json`, `*.txt`)
- Python artifacts (`__pycache__/`, `*.pyc`, `*.egg-info/`)
- Virtual environments (`venv/`, `env/`, `.venv`)
- IDE files (`.vscode/`, `.idea/`, `*.swp`)
- OS files (`.DS_Store`, `Thumbs.db`)

## Development Workflows

### Git Workflow

This project follows a **feature branch + PR workflow**:

1. **Feature branches** named `feature/<description>` (e.g., `feature/site-tree-structure`)
2. **Pull Requests** merge features into main branch
3. **Commit messages** reference issues with "fixes #N" syntax
4. **PR titles** follow format: descriptive message + issue reference

**Example commit history:**
```
9416f86 Ajoute la colorisation de l'arborescence (fixes #5) (#6)
947b483 Merge pull request #4 from xavier-quadratic/feature/site-tree-structure Closes #2
3ddcc73 Ajoute la visualisation de l'arborescence du site (fixes #3)
53c4c70 Implémente le scraper web pour quadratic-labs.com (fixes #1)
```

### Commit Message Conventions

- **Language:** French (project is French-speaking)
- **Format:** Imperative mood + issue reference
- **Pattern:** `[Action] [description] (fixes #[issue_number])`
- **Example:** `Ajoute la colorisation de l'arborescence (fixes #5)`

Common action verbs:
- `Ajoute` - Add new feature
- `Implémente` - Implement functionality
- `Corrige` - Fix bug
- `Met à jour` - Update existing feature
- `Supprime` - Remove/delete

### Testing & Quality

**Current state:** No automated tests present in repository.

**Manual testing approach:**
1. Run scraper: `python scraper.py`
2. Verify output files generated
3. Check terminal output for colored tree
4. Validate JSON structure

**When adding features:**
- Test with rate limiting to avoid overloading target server
- Verify color output on different terminals
- Test both module and CLI usage modes
- Validate JSON/TXT export formats

### Making Changes

**Before modifying code:**
1. Read the relevant section of `scraper.py`
2. Understand data structures (`url_hierarchy`, `url_depth`)
3. Consider impact on both CLI and library usage
4. Preserve French documentation in docstrings and comments

**Typical change workflow:**
```bash
# 1. Create feature branch
git checkout -b feature/new-feature-name

# 2. Make changes to scraper.py
# 3. Test manually
python scraper.py

# 4. Update readme.md with new feature documentation
# 5. Commit with French message
git commit -m "Ajoute [description] (fixes #N)"

# 6. Push and create PR
git push -u origin feature/new-feature-name
```

## Code Conventions

### Style Guidelines

1. **Language:**
   - Code comments: French
   - Docstrings: French
   - Variable names: English (standard Python convention)
   - User-facing output: French
   - Documentation (README): French

2. **Formatting:**
   - Indentation: 4 spaces
   - Line length: Generally < 100 characters (not strictly enforced)
   - Quotes: Single quotes for strings, double quotes for human-readable text
   - Imports: Standard library → Third-party → Local (with blank lines between groups)

3. **Naming Conventions:**
   - Classes: `PascalCase` (e.g., `QuadraticLabsScraper`)
   - Methods/functions: `snake_case` (e.g., `get_links_from_page`)
   - Constants: `UPPER_CASE` (though few used in this project)
   - Private methods: Prefix with `_` (e.g., `_print_tree_recursive`, `_get_color_for_depth`)

4. **Type Hints:**
   - Used consistently throughout (Python 3.7+ style)
   - Import types from `typing` module
   - Example: `def scrape(self, max_pages: int = 100, delay: float = 0.5) -> List[str]:`

5. **Logging:**
   - Use `logging` module (configured at module level, lines 19-23)
   - Log levels: INFO for progress, ERROR for exceptions
   - Format: `%(asctime)s - %(levelname)s - %(message)s`

### Scraping Best Practices (Enforced in Code)

1. **Rate Limiting:** Default 0.5s delay between requests (`time.sleep(delay)`)
2. **User-Agent:** Identifies as Mozilla browser (line 39)
3. **Domain Filtering:** Only scrapes same-domain URLs (`is_valid_url()`)
4. **URL Normalization:** Removes fragments and trailing slashes
5. **Error Handling:** Catches `RequestException`, logs errors, continues scraping
6. **Session Management:** Uses `requests.Session()` for connection pooling

## Common Tasks for AI Assistants

### Task 1: Add New Export Format

**Files to modify:**
- `scraper.py` - Add new `save_to_*()` method
- `readme.md` - Document new format in "Format de sortie" section

**Example locations:**
- Add method after `save_tree_to_json()` (after line 205)
- Update `main()` to call new method (around line 323)

### Task 2: Add New Scraping Feature

**Key considerations:**
- Update `scrape()` method (lines 84-159) for new data collection
- Modify data structures if tracking new information
- Update `save_*()` methods to export new data
- Add French docstrings explaining functionality
- Update README with usage examples

### Task 3: Modify Color Scheme

**Location:** `_get_color_for_depth()` method (lines 207-221)

**Available colors from colorama:**
- `Fore`: BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE
- `Style`: BRIGHT, DIM, NORMAL, RESET_ALL
- Remember: Check `self.use_colors` flag before applying

### Task 4: Add Command-Line Arguments

**Current:** No CLI argument parsing (uses hardcoded defaults in `main()`)

**To add:**
1. Import `argparse` module
2. Create parser in `main()` before line 292
3. Pass arguments to `QuadraticLabsScraper()` and `scrape()`
4. Update README usage section

### Task 5: Fix Bugs or Handle Edge Cases

**Common areas:**
- URL validation (`is_valid_url()`, `normalize_url()`)
- Connection errors (`get_links_from_page()` try/except)
- Tree building logic (parent-child relationships in `scrape()`)
- Empty results handling

**Debugging:**
- Logger outputs to console (level=INFO)
- Check `self.visited_urls` vs `self.found_urls` for coverage
- Verify `url_hierarchy` structure with `print_tree()` output

## Dependencies & Setup

### Installation

```bash
# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### Running the Scraper

**As standalone script:**
```bash
python scraper.py
```

**As imported module:**
```python
from scraper import QuadraticLabsScraper

scraper = QuadraticLabsScraper(base_url="https://example.com", use_colors=True)
pages = scraper.scrape(max_pages=50, delay=1.0)
scraper.print_tree()
scraper.save_to_json("output.json")
```

## Understanding the Algorithm

### BFS Traversal (Breadth-First Search)

The scraper uses BFS to explore the site level-by-level:

```
1. Start with base_url at depth 0
2. Extract all links from current page
3. Add unvisited links to queue with depth+1
4. Process next URL in queue (FIFO)
5. Repeat until max_pages reached or queue empty
```

**Key data flow:**
```python
to_visit = [(url, parent_url, depth), ...]  # Queue
↓
current_url = to_visit.pop(0)  # Process next
↓
links = get_links_from_page(current_url)  # Discover children
↓
to_visit.append((link, current_url, depth+1))  # Add to queue
↓
url_hierarchy[current_url]['children'].append(link)  # Track relationship
```

### Tree Structure

```python
url_hierarchy = {
    'https://example.com': {
        'parent': None,
        'children': ['https://example.com/about', 'https://example.com/contact'],
        'depth': 0
    },
    'https://example.com/about': {
        'parent': 'https://example.com',
        'children': ['https://example.com/about/team'],
        'depth': 1
    },
    ...
}
```

## Project History & Context

### Completed Issues

- **Issue #1:** Initial scraper implementation
- **Issue #3:** Site tree structure visualization
- **Issue #5:** Tree colorization

### Evolution

1. **v1 (commit 53c4c70):** Basic scraper - flat URL list
2. **v2 (commit 3ddcc73):** Added hierarchical tree + depth tracking
3. **v3 (commit 9416f86):** Added colorization for better visualization

### Design Decisions

- **Single-file design:** Simple project, no need for multiple modules
- **Class-based:** Allows both CLI and library usage
- **BFS not DFS:** Provides better depth organization for tree visualization
- **Colorama:** Cross-platform color support (important for Windows)
- **French documentation:** Project targets French-speaking users

## Troubleshooting

### Common Issues

1. **Import errors:** Ensure `pip install -r requirements.txt` was run
2. **Connection timeouts:** Target site may be slow/down - handled gracefully
3. **No colors on Windows:** Colorama should handle this; check `use_colors=True`
4. **Duplicate URLs:** Should be handled by `normalize_url()` + set deduplication

### When Modifying Code

**Always verify:**
- URL normalization still works (fragments removed, trailing slashes)
- Domain filtering prevents external links
- Parent-child relationships maintained correctly
- Both CLI and import usage modes work
- Output files use UTF-8 encoding (ensure_ascii=False)

## Language & Localization

**Important:** This is a French-language project.

**When creating commits/documentation:**
- Write commit messages in French
- Add French comments to code
- Update README.md in French
- Use French in log messages and user output
- Keep variable/method names in English (Python convention)

**Example:**
```python
def get_links_from_page(self, url: str) -> Set[str]:
    """Extrait tous les liens d'une page"""  # French docstring
    links = set()  # English variable name
    logger.info(f"Scraping: {url}")  # English/neutral log
    ...
```

## Additional Notes for AI Assistants

### Best Practices When Working on This Repo

1. **Always read before modifying:** Use Read tool on scraper.py before changes
2. **Test changes:** Run `python scraper.py` after modifications
3. **Preserve structure:** Maintain single-file architecture unless explicitly asked to refactor
4. **French-first:** All user-facing text and documentation in French
5. **Respect rate limits:** Don't reduce default delay below 0.5s without good reason
6. **Maintain backwards compatibility:** Code may be used as library by other projects

### When Asked to Add Features

**Checklist:**
- [ ] Implement in `QuadraticLabsScraper` class
- [ ] Add French docstring
- [ ] Update `main()` if needed for CLI
- [ ] Test both CLI and import usage
- [ ] Update readme.md with French documentation
- [ ] Consider impact on output files
- [ ] Maintain type hints
- [ ] Follow existing code style

### Quick Reference: Key Line Numbers

- Imports: 6-13
- Logging config: 19-23
- Class definition: 26
- Constructor: 29-40
- Main scraping loop: 109-156
- Tree printing: 223-271
- CLI entry point: 289-335

---

**Document Version:** 1.0
**Last Updated:** 2025-11-24
**Maintainer:** AI-generated for claude-code workflow
