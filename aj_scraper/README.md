# Scraper d'Annonces d'Administrateurs Judiciaires

SystÃ¨me complet pour scraper et filtrer les annonces d'entreprises Ã  cÃ©der publiÃ©es par les administrateurs judiciaires en Ãle-de-France, spÃ©cialisÃ© dans les secteurs de l'informatique, data et conseil.

## ğŸ¯ Objectif

Ce projet permet de :
1. **Extraire** la liste des administrateurs judiciaires en Ãle-de-France depuis l'annuaire officiel
2. **Analyser** leurs sites web pour identifier les pages contenant des annonces de vente
3. **Collecter** et **filtrer** automatiquement les annonces pertinentes selon :
   - **Secteur** : informatique, data, conseil, numÃ©rique, etc.
   - **RÃ©gion** : Ãle-de-France (dÃ©partements 75, 77, 78, 91, 92, 93, 94, 95)

## ğŸ“‹ PrÃ©requis

- Python 3.7 ou supÃ©rieur
- pip (gestionnaire de paquets Python)

## ğŸš€ Installation

### 1. Cloner le repository

```bash
git clone <votre-repo>
cd test_claude/aj_scraper
```

### 2. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 3. Configuration (optionnelle)

Vous pouvez modifier les paramÃ¨tres dans `config.py` :

```python
# RÃ©gion cible
TARGET_REGION = "Ãle-de-France"
TARGET_DEPARTMENTS = ["75", "77", "78", "91", "92", "93", "94", "95"]

# Secteurs d'activitÃ©
TARGET_SECTORS = [
    "informatique",
    "data",
    "conseil",
    "numÃ©rique",
    # ... ajoutez vos secteurs
]

# ParamÃ¨tres de scraping
DELAY_BETWEEN_REQUESTS = 1.0  # secondes
MAX_RETRIES = 3
```

## ğŸ’» Utilisation

### Mode simple (pipeline complet)

```bash
python main.py
```

Cette commande exÃ©cute les 3 phases automatiquement :
1. Extraction des administrateurs judiciaires
2. Analyse des sites web
3. Extraction et filtrage des annonces

### Mode avec Selenium

Si le site bloque les requÃªtes HTTP classiques, utilisez Selenium :

```bash
python main.py --selenium
```

> âš ï¸ **Note** : Selenium nÃ©cessite Chrome/Chromium installÃ© sur votre systÃ¨me.

### ExÃ©cution phase par phase

```bash
# Phase 1 uniquement (extraction des administrateurs)
python main.py --phase 1

# Phase 2 uniquement (analyse des sites)
python main.py --phase 2

# Phase 3 uniquement (extraction des annonces)
python main.py --phase 3
```

### Reprendre depuis une phase

Pour Ã©viter de re-scraper les donnÃ©es dÃ©jÃ  collectÃ©es :

```bash
# Ignorer la phase 1, utiliser les donnÃ©es existantes
python main.py --skip-phase1

# Ignorer les phases 1 et 2
python main.py --skip-phase1 --skip-phase2
```

## ğŸ“‚ Structure du projet

```
aj_scraper/
â”œâ”€â”€ config.py                  # Configuration centrale
â”œâ”€â”€ scraper_annuaire.py       # Phase 1: Scraping de l'annuaire
â”œâ”€â”€ scraper_sites.py          # Phase 2: Analyse des sites
â”œâ”€â”€ scraper_annonces.py       # Phase 3: Extraction des annonces
â”œâ”€â”€ filters.py                # Filtrage par secteur et rÃ©gion
â”œâ”€â”€ main.py                   # Script principal
â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â”œâ”€â”€ README.md                 # Cette documentation
â”œâ”€â”€ data/                     # DonnÃ©es temporaires
â”œâ”€â”€ output/                   # RÃ©sultats (JSON, CSV)
â””â”€â”€ logs/                     # Logs d'exÃ©cution
```

## ğŸ“Š Fichiers de sortie

Tous les rÃ©sultats sont sauvegardÃ©s dans le dossier `output/` :

### 1. `administrateurs_idf.json`
Liste des administrateurs judiciaires en Ãle-de-France avec leurs coordonnÃ©es et sites web.

```json
{
  "region": "Ãle-de-France",
  "total": 125,
  "administrateurs": [
    {
      "nom": "Cabinet Example",
      "adresse": "75008 Paris",
      "site_web": "https://example.fr",
      "telephone": "01 23 45 67 89",
      "email": "contact@example.fr",
      "departement": "75"
    }
  ]
}
```

### 2. `sites_analysis.json`
Analyse des sites web et pages d'annonces dÃ©tectÃ©es.

```json
[
  {
    "base_url": "https://example.fr",
    "accessible": true,
    "pages_annonces": [
      "https://example.fr/annonces",
      "https://example.fr/ventes"
    ],
    "structure_detected": {
      "has_table": true,
      "item_tag": "tr",
      "pagination": true
    }
  }
]
```

### 3. `annonces_brutes.json`
Toutes les annonces extraites avant filtrage.

### 4. `annonces_filtrees.json` et `annonces_filtrees.csv`
Annonces filtrÃ©es selon vos critÃ¨res (secteur, rÃ©gion).

```json
{
  "total": 42,
  "extracted_at": "2025-11-24T10:00:00",
  "annonces": [
    {
      "titre": "Entreprise de dÃ©veloppement web",
      "description": "SociÃ©tÃ© spÃ©cialisÃ©e en dÃ©veloppement web...",
      "secteur": "Informatique",
      "localisation": "75008 Paris",
      "prix": "150000",
      "date_publication": "01/11/2025",
      "reference": "AJ-2025-001",
      "url_details": "https://example.fr/annonce/001",
      "contact": "contact@example.fr"
    }
  ]
}
```

## ğŸ”§ Architecture des modules

### 1. `scraper_annuaire.py` - Extraction de l'annuaire

```python
from scraper_annuaire import AnnuaireScraper

# Utilisation basique
scraper = AnnuaireScraper()
administrateurs = scraper.scrape_all_pages()
scraper.save_results()

# Avec Selenium
scraper = AnnuaireScraper(use_selenium=True)
administrateurs = scraper.scrape_all_pages()
```

**FonctionnalitÃ©s** :
- Scraping de l'annuaire https://www.cnajmj.fr/annuaire/
- Filtrage automatique par rÃ©gion (Ãle-de-France)
- Gestion de la pagination
- Support requests + Selenium
- Extraction : nom, adresse, site web, tÃ©lÃ©phone, email

### 2. `scraper_sites.py` - Analyse des sites

```python
from scraper_sites import SiteAnalyzer

analyzer = SiteAnalyzer()
results = analyzer.analyze_site("https://example-aj.fr")

# RÃ©sultat
{
  "accessible": True,
  "pages_annonces": ["https://example-aj.fr/annonces"],
  "structure_detected": {...}
}
```

**FonctionnalitÃ©s** :
- DÃ©tection automatique des pages d'annonces
- 3 stratÃ©gies de recherche :
  1. Analyse du menu de navigation
  2. Recherche par mots-clÃ©s dans le contenu
  3. Crawling intelligent du site
- DÃ©tection de la structure des pages (tables, listes, cartes)

### 3. `scraper_annonces.py` - Extraction des annonces

```python
from scraper_annonces import AnnonceScraper

scraper = AnnonceScraper()
annonces = scraper.extract_annonces_from_page(url)

# Sauvegarde
scraper.save_annonces(annonces)
scraper.export_to_csv(annonces)
```

**FonctionnalitÃ©s** :
- Extraction adaptative selon la structure dÃ©tectÃ©e
- Parsing de tables, listes, cartes HTML
- Extraction : titre, description, prix, localisation, contact, dates
- Export JSON et CSV

### 4. `filters.py` - Filtrage intelligent

```python
from filters import AnnonceFilter

filtre = AnnonceFilter()

# Filtrage complet
filtered = filtre.apply_all_filters(
    annonces,
    filter_sector=True,        # Filtre par secteur
    filter_location=True,       # Filtre par rÃ©gion
    min_price=50000,           # Prix minimum
    max_price=500000,          # Prix maximum
    custom_keywords=["saas"],  # Mots-clÃ©s additionnels
    exclude_keywords=["restaurant"]  # Exclusions
)

# Statistiques
stats = filtre.get_statistics(filtered)
```

**FonctionnalitÃ©s** :
- Filtrage par secteur d'activitÃ© (avec mots-clÃ©s Ã©tendus)
- Filtrage par localisation (codes postaux + noms)
- Filtrage par prix
- Filtrage par mots-clÃ©s personnalisÃ©s
- Statistiques dÃ©taillÃ©es

## ğŸ¨ Exemples d'utilisation

### Exemple 1 : Recherche ciblÃ©e

```python
from main import AJScraperPipeline
from filters import AnnonceFilter

# ExÃ©cute le pipeline
pipeline = AJScraperPipeline()
pipeline.run()

# Filtre supplÃ©mentaire pour des mots-clÃ©s spÃ©cifiques
filtre = AnnonceFilter()
annonces_saas = filtre.filter_by_keywords(
    pipeline.filtered_annonces,
    keywords=["saas", "cloud", "api"]
)

print(f"Annonces SaaS trouvÃ©es: {len(annonces_saas)}")
```

### Exemple 2 : Analyse d'un seul site

```python
from scraper_sites import SiteAnalyzer
from scraper_annonces import AnnonceScraper

# Analyse
analyzer = SiteAnalyzer()
result = analyzer.analyze_site("https://example-aj.fr")

# Extraction si des annonces sont trouvÃ©es
if result['pages_annonces']:
    scraper = AnnonceScraper()
    annonces = scraper.extract_annonces_from_multiple_pages(
        result['pages_annonces']
    )
    print(f"TrouvÃ© {len(annonces)} annonces")
```

### Exemple 3 : Scraping pÃ©riodique

```bash
# Script cron pour exÃ©cuter chaque jour
0 9 * * * cd /path/to/aj_scraper && python main.py --skip-phase1 --skip-phase2
```

Ce script rÃ©-extrait uniquement les nouvelles annonces chaque jour.

## âš ï¸ Limitations et adaptations nÃ©cessaires

### 1. Structure du site de l'annuaire

Le module `scraper_annuaire.py` contient des **heuristiques gÃ©nÃ©riques**. Vous devrez probablement adapter les mÃ©thodes suivantes aprÃ¨s avoir inspectÃ© le HTML rÃ©el du site :

```python
# Dans scraper_annuaire.py, ligne 142
def parse_annuaire_page(self, html: str) -> List[Dict]:
    # Ã€ ADAPTER selon la structure rÃ©elle du site
    entries = soup.find_all('div', class_=['entry', 'result', ...])
```

### 2. Protection anti-scraping

Si vous rencontrez des erreurs 403 ou des blocages :
- Utilisez l'option `--selenium`
- Installez Chrome/Chromium : `apt-get install chromium-browser chromium-driver`
- Augmentez les dÃ©lais dans `config.py`
- Utilisez des proxies ou un service comme ScraperAPI

### 3. Structure des pages d'annonces

Les sites d'administrateurs judiciaires ont tous des structures diffÃ©rentes. Le systÃ¨me utilise des heuristiques pour s'adapter, mais vous pourriez devoir :
- Inspecter manuellement quelques sites
- Adapter les sÃ©lecteurs CSS dans `scraper_annonces.py`

## ğŸ› DÃ©pannage

### Erreur 403 Forbidden

```bash
# Solution 1: Utiliser Selenium
python main.py --selenium

# Solution 2: Augmenter le dÃ©lai
# Dans config.py: DELAY_BETWEEN_REQUESTS = 2.0
```

### ChromeDriver introuvable

```bash
# Linux (Debian/Ubuntu)
sudo apt-get install chromium-browser chromium-driver

# macOS
brew install --cask chromedriver

# Windows
# TÃ©lÃ©chargez depuis https://chromedriver.chromium.org/
```

### Aucune annonce trouvÃ©e

1. VÃ©rifiez que les sites sont accessibles
2. Inspectez `output/sites_analysis.json` pour voir les pages dÃ©tectÃ©es
3. Testez manuellement un site :

```python
from scraper_sites import SiteAnalyzer
analyzer = SiteAnalyzer()
result = analyzer.analyze_site("https://example-aj.fr")
print(result)
```

## ğŸ“ Logs et debugging

Les logs dÃ©taillÃ©s sont affichÃ©s en temps rÃ©el. Pour sauvegarder dans un fichier :

```bash
python main.py 2>&1 | tee output.log
```

Pour plus de dÃ©tails, modifiez le niveau de log dans chaque module :

```python
logging.basicConfig(level=logging.DEBUG)  # Au lieu de INFO
```

## ğŸ” ConsidÃ©rations lÃ©gales

âš ï¸ **Important** :
- Respectez les conditions d'utilisation des sites scrapÃ©s
- Respectez les dÃ©lais entre requÃªtes (configurÃ©s par dÃ©faut)
- Les donnÃ©es extraites sont publiques mais vÃ©rifiez leur utilisation autorisÃ©e
- Certains sites peuvent interdire le scraping automatique dans leurs CGU

## ğŸ¤ Contribution

Pour amÃ©liorer ce projet :
1. Testez avec diffÃ©rents sites d'administrateurs judiciaires
2. Signalez les bugs et structures non supportÃ©es
3. Proposez des amÃ©liorations pour la dÃ©tection automatique

## ğŸ“„ Licence

Ce projet est fourni tel quel, pour usage Ã©ducatif et professionnel.

## ğŸ†˜ Support

En cas de problÃ¨me :
1. VÃ©rifiez la section DÃ©pannage
2. Consultez les logs dÃ©taillÃ©s
3. Testez chaque module indÃ©pendamment
4. Inspectez les fichiers HTML sauvegardÃ©s

## ğŸ—ºï¸ Roadmap

AmÃ©liorations futures possibles :
- [ ] Support d'autres rÃ©gions
- [ ] Interface web pour visualiser les rÃ©sultats
- [ ] Notifications par email des nouvelles annonces
- [ ] Base de donnÃ©es pour historiser les annonces
- [ ] API REST pour interroger les donnÃ©es
- [ ] Dashboard avec statistiques en temps rÃ©el

---

**Bon scraping ! ğŸš€**
